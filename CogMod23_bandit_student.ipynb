{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2450dd1e",
   "metadata": {},
   "source": [
    "# Reinforcement Learning in bandit tasks and gridworlds\n",
    "\n",
    "Franz Wurm<br>\n",
    "<sub>Leiden University<sub><br><br>  \n",
    "    \n",
    "This is the code for the practical session in the course \"2324-S1 Cognitive Modelling: How to build a brain\" [(link to Brightspace)](https://brightspace.universiteitleiden.nl/d2l/home/240341)<br>   \n",
    "Date: 09.10.2023<br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8184f326",
   "metadata": {},
   "source": [
    "**Useful references**\n",
    "- Sutton, R. S., & Barto, A. G. (2018). Reinforcement learning: An introduction. MIT press."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23253e22",
   "metadata": {},
   "source": [
    "## 1. General information\n",
    "\n",
    "### Goal of this practical\n",
    "\n",
    "In this practical, you are going to implement a reinforcement learning agent for a standard decision-making problem. Our agent will be able to learn the value of states and actions in the environment simply by observing their associated rewards and punishments. As discussed in the lecture, this resembles the principles of classical and operant conditioning. It is a efficient way of learning that leads to simple, habitual behavioral patterns. \n",
    "\n",
    "Step by step, we will discuss the different necessary components to realize such a reinforcement learning agent.\n",
    "\n",
    "### Setting up this notebook \n",
    "\n",
    "This notebook contains all necessary information for the first practical session.\n",
    "\n",
    "I recommend to download the file and save it to a separate folder. Optimally, this folder is easily accessible (e.g. on your desktop) or implemented in a preexisting folder structure (e.g. MyBachelor>CogMod>PracticalRL).\n",
    "\n",
    "I also recommend to work with copies. That means, you should not work on the original file, but rather work on copies. This makes sure, that you do not delete important information and always have a basis to go back to. Additionaly, you could implement version control, meaning you save your work to a new file from time to time (e.g., filename_v1, filename_v2). This makes sure you dont lose too much progress in case your computer shuts down or you forgot to press the save button.\n",
    "\n",
    "The exercise is constructed to be completed chronologically, i.e. from top to bottom. There will be blocks with text, such as the text that you are currently reading, as well as blocks of code, where you can execute commands and computations.\n",
    "\n",
    "The whole tutorial is written in Python3.\n",
    "\n",
    "For working on the excersises of the practical I suggest you use [Google Colab](https://colab.research.google.com/) or [Jupyter-notebook](https://jupyter.org/). While Google Colab is straight foward to use, Jupyter-notebook might require you to install additional software (e.g., [Anaconda](https://anaconda.org/))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f7b1ebc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# first, let's import some packages that we might need\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40f89d82",
   "metadata": {},
   "source": [
    "## 2. k-armed bandit task\n",
    "\n",
    "In the [bandit problem](https://en.wikipedia.org/wiki/Multi-armed_bandit), the agent is faced with a choice between multiple options. Named after slot machines in a casino, our one and two-armed versions in the lecture helped to demonstrate the two core principles that allow the agent to achieve its goal of reward maximzation. \n",
    "\n",
    "The first thing we will do is to create a _function_ that generates samples from playing such a bandit. \n",
    "\n",
    "The function below samples reward drawn from a uniform distribution based on prespecified reward probabilities. As you can see, the function takes input parameters that dictate the number of trials to sample (<code>n_trials</code>) and the reward probablities (<code>probs</code>). \n",
    "\n",
    "The function returns and array with reward samples (<code>reward_samples</code>) and the reward probabilities (<code>reward_probs</code>) that generated it. Please note, that specifying more than one reward probabilities (e.g., <code>[0.1, 0,9]</code>) will sample two independent bandits and return them as a matrix.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e993fb45",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_uniform_bandit(n_trials,probs):\n",
    "    \n",
    "    #preallocation\n",
    "    reward_samples = np.empty((len(probs),n_trials))\n",
    "    reward_probs = np.empty((len(probs),n_trials))\n",
    "    \n",
    "    for i in np.arange(len(probs)):\n",
    "        \n",
    "        #get samples for each bandit\n",
    "        reward_samples[i,:] = np.random.binomial(1,probs[i],n_trials)\n",
    "        reward_probs[i,:] = probs[i]\n",
    "        \n",
    "    \n",
    "    return reward_samples, reward_probs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "324fc7fa",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\" role=\"alert\">\n",
    "<h2> Let's try it out! </h2><br>\n",
    "\n",
    "Below you should generate two bandits with each 100 trials and a reward probability of 80% and 20%.\n",
    "    \n",
    ">HINT: Use the <code>print</code> function and the matplotlib methods, loaded as <code>plt</code>.\n",
    "</div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d12ca482",
   "metadata": {},
   "outputs": [],
   "source": [
    "?np.mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "91dc46fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 1. 1. 1. 0. 1. 0. 1. 0. 1.]\n",
      " [0. 0. 1. 0. 0. 0. 1. 0. 1. 0.]]\n",
      "mean: [0.6 0.3]\n"
     ]
    }
   ],
   "source": [
    "## TODO: print and plot results\n",
    "\n",
    "n_trials = 10\n",
    "prob = [0.8, 0.2]\n",
    "\n",
    "reward_samples, reward_probs = generate_uniform_bandit(n_trials,prob)\n",
    "\n",
    "print(reward_samples)\n",
    "print(f'mean: {np.mean(reward_samples,axis=1)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e752eec",
   "metadata": {},
   "source": [
    "As discussed in the lecture, the two principles of reinforcement learning are\n",
    "- the learning rule: updating of expectations based on observations \n",
    "- the decision rule: taking actions based on expectations\n",
    "\n",
    "\n",
    "## 3. Learning rule \n",
    "\n",
    "The learning rule should help our agent to build up expectations about its environment.\n",
    "\n",
    "In this section, we are going to demonstrate why a weighted-average method outperforms a standard average method in changing environments.\n",
    "\n",
    "### Improving learning... by calculating the average\n",
    "\n",
    "As a first step, let us have a look at the performance of the averaging method for incremental learning in a stable environment.\n",
    "\n",
    "We specify the learning rule for incremental averaging in acordance with the formula introduced in the lecture.\n",
    "\n",
    "$\\LARGE V_{t+1} = V_{t} + \\frac{1} {t}(R_{t} - V_{t})$,\n",
    "\n",
    ">where $V$ is the estimated average value,<br>\n",
    ">and $t$ is the current time step.<br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "fca64d04",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "n_trials = 1000\n",
    "prob = [0.7]\n",
    "\n",
    "reward_samples, reward_probs = generate_uniform_bandit(n_trials,prob)\n",
    "#a = np.random.binomial(1,0.7,10)\n",
    "\n",
    "\n",
    "mean_avg = np.empty((1,n_trials+1))\n",
    "for iT in np.arange(n_trials):\n",
    "    if iT==0:\n",
    "        mean_avg[0,0] = 0.5\n",
    "    mean_avg[0,iT+1] = mean_avg[0,iT] + (1/(iT+1))*(reward_samples[0,iT]-mean_avg[0,iT])\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2df5654",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\" role=\"alert\">\n",
    "<h2> Challenge </h2><br>\n",
    "\n",
    "Show that the incremental average mean indeed converged to the true mean.\n",
    "    \n",
    ">HINT: Use the <code>print</code> function and the matplotlib methods, loaded as <code>plt</code>.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "9e9ad4e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "## TODO: print and plot results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0f6807f",
   "metadata": {},
   "source": [
    "### ...in changing environments\n",
    "\n",
    "Now that we have seen that the average method of incremental learning does a good job in estimating the average on a step-by-step basis, we are going to extend this example to cover changing environments. \n",
    "\n",
    "As shown in the lecture (slide 12), the environment undergoes sudden changes in reward probabilities.\n",
    "\n",
    "We also implement the weighted-average method as introduced in the lecture.\n",
    "\n",
    "$\\LARGE V_{t+1} = V_{t} + \\alpha(R_{t} - V_{t})$,\n",
    "\n",
    ">where $\\alpha$ is the learning rate.<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82b3573e",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\" role=\"alert\">\n",
    "<h2> Challenge </h2><br>\n",
    "\n",
    "In the code box below, the method for the average method is already implemented.\n",
    "\n",
    "Add the weighted average method and plot the results.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "8c8ce71c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## TODO: Add weighted-average method\n",
    "\n",
    "n_trials = 1000\n",
    "reward_prob = [0.7, 0.3, 0.7, 0.3, 0.7, 0.3]\n",
    "#reward_prob = [1, 0, 1, 0]\n",
    "\n",
    "\n",
    "reward_samples, reward_probs = generate_uniform_bandit(n_trials,reward_prob)\n",
    "#print(reward_samples)\n",
    "\n",
    "# Simulate a bandit with changing reward probabilities by\n",
    "# flattening a m-by-n matrix into a 1-by-m*n vector \n",
    "reward_samples = np.asarray(reward_samples)\n",
    "reward_samples = reward_samples.flatten()\n",
    "reward_probs = np.asarray(reward_probs)\n",
    "reward_probs = reward_probs.flatten()\n",
    "# fyi, this problem could also be solved by multiple loops\n",
    "\n",
    "mean_avg = np.empty((1,len(reward_prob)*n_trials+1))\n",
    "for iT in np.arange(len(reward_prob)*n_trials):    \n",
    "    if iT==0: #initialize at time point 0\n",
    "        mean_avg[0,0] = 0.5  \n",
    "    #update estimated values    \n",
    "    mean_avg[0,iT+1] = mean_avg[0,iT] + (1/(iT+1))*(reward_samples[iT]-mean_avg[0,iT])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "821cb217",
   "metadata": {},
   "outputs": [],
   "source": [
    "## TODO: Plotting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "369a1fec",
   "metadata": {},
   "source": [
    "## 3. Decision rule\n",
    "\n",
    "So far, we have only dealt with prediction problems. However, the most powerful application of reinforcement learning lies in solving control problems. \n",
    "\n",
    "The decision rule (or policy) defines the agent's way of behaving at a given time or state. This is and extension of the previous prediction problem, as we now also consider actions given each state. Put differently, the policy is a mapping between states and actions and it corresponds to what psychologists sometimes call a stimulus-response association.\n",
    "\n",
    "\n",
    "### Improving decision making... by exploration\n",
    "\n",
    "In order to replicate the results for the improvement of decision-making as shown in the lecture (slide 15), we first need to set up a new bandit function.\n",
    "\n",
    "This new bandit function again takes two input parameters and give two output parameter. In contrast to the previous bandit, the current bandit will draw its reward from a normal distribution. So we will not specify reward probabilities but rather <code>reward_means</code> as the second argument.\n",
    "\n",
    "\n",
    "<div class=\"alert alert-warning\" role=\"alert\">\n",
    "<h2> Challenge </h2><br>\n",
    "\n",
    "Add a function to the routine below that does sample rewards from a normal distribution with a mean of <code>means</code> and a standard deviation of <code>1</code>. <br>\n",
    "<br>\n",
    "    \n",
    ">HINT: Replace the <code>???</code> in the box below with the appropriate function.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1398f119",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_normal_bandit(n_trials,means):\n",
    "    \n",
    "    # set the same random seed so we replicate the same random values\n",
    "    #np.random.seed(2)\n",
    "    \n",
    "    #preallocation\n",
    "    reward_samples = np.empty((len(means),n_trials))\n",
    "    reward_means = np.empty((len(means),n_trials))\n",
    "    \n",
    "    for i in np.arange(len(means)):\n",
    "    \n",
    "        #get samples for each bandit\n",
    "        reward_samples[i,:] = ???\n",
    "        reward_means[i,:] = means[i]\n",
    "        \n",
    "    \n",
    "    return reward_samples, reward_means\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1deacbcc",
   "metadata": {},
   "source": [
    "During the lecture, we introduced a few different decision rules and of course the list in not exhaustive. Below you find the most commonly used methods:\n",
    "- random method\n",
    "- greedy method\n",
    "- e-greedy method\n",
    "- softmax method\n",
    "\n",
    "Let's first implement the different methods for action selection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4199f99a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def action_random(values):\n",
    "    nOptions = len(values)\n",
    "    actions = np.arange(nOptions)\n",
    "    \n",
    "    action = np.random.choice(actions)\n",
    "    \n",
    "    return action\n",
    "\n",
    "def action_greedy(values):\n",
    "    nOptions = len(values)\n",
    "    actions = np.arange(nOptions)\n",
    "    \n",
    "    ties = np.array(values[:,0] == np.max(values[:,0]))\n",
    "    action = np.random.choice(actions[ties]) \n",
    "    \n",
    "    return action\n",
    "\n",
    "def action_epsilon(values,epsilon):\n",
    "    nOptions = len(values)\n",
    "    actions = np.arange(nOptions)\n",
    "    \n",
    "    if (np.random.random() < epsilon):\n",
    "        action = action_random(values)\n",
    "    else:\n",
    "        action = action_greedy(values)\n",
    "    \n",
    "    return action\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "147d66a4",
   "metadata": {},
   "source": [
    "Let's make a few assumptions for our simulation first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c33ca46e",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_agents = 1000\n",
    "n_trials = 1000\n",
    "n_bandits = 10\n",
    "#reward_means = [-1.5,-1,-0.5,0,0.5,1,1.5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7850cf33",
   "metadata": {},
   "source": [
    "#### Greedy agents\n",
    "\n",
    "In the following code block I have set up a number of greedy agents.\n",
    "\n",
    "In accordance with the lecture, both the average for the rewards (<code>rewards_greedy</code>) and the percentage of optimal decisions (<code>optimum_greedy</code>) will be saved for plotting.\n",
    "\n",
    "Please note, that we implement the _average methode_ as the learning rule. As this is a stable environment, the properties of the average method guarantee that our estimates converge to the optimum/true value (at least with an appropriate action selection method...)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7602050d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## greedy action selection\n",
    "\n",
    "rewards_greedy = np.empty((n_agents,n_trials))\n",
    "optimum_greedy = np.empty((n_agents,n_trials))\n",
    "for iA in np.arange(n_agents):\n",
    "\n",
    "    reward_means = np.random.normal(0,1,(n_bandits,1))\n",
    "    optimal_action = np.argmax(reward_means)\n",
    "    reward_samples, reward_probs = generate_normal_bandit(n_trials,reward_means)\n",
    "\n",
    "    #initialize the estimated values for each bandit\n",
    "    values = np.zeros((len(reward_means),1))\n",
    "    action_count = np.zeros((len(reward_means),1))\n",
    "    \n",
    "    for iT in np.arange(n_trials):    \n",
    "        #chose action\n",
    "        action = action_greedy(values)\n",
    "        action_count[action,0] =  action_count[action,0]+1\n",
    "        #calculate prediction error\n",
    "        PE = reward_samples[action,iT]-values[action,0]\n",
    "        #update value\n",
    "        values[action,0] = values[action,0] + (1/(iT+1))*PE\n",
    "        #store reward & optimal decision\n",
    "        rewards_greedy[iA,iT] = reward_samples[action,iT]\n",
    "        if action == optimal_action:\n",
    "            optimum_greedy[iA,iT] = 1\n",
    "        else:\n",
    "            optimum_greedy[iA,iT] = 0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8082491",
   "metadata": {},
   "source": [
    "#### $\\epsilon$-greedy action selection\n",
    "\n",
    "Basically, the $\\epsilon$-greedy agent is identical to the greedy agent with the the minor detail that on 100*$\\epsilon$% of the trials a random action will be chosen.\n",
    "\n",
    "You find the code for $\\epsilon$-greedy actions selection already above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a68e231a",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "epsilon = 0.1\n",
    "\n",
    "rewards_egreedy = np.empty((n_agents,n_trials))\n",
    "optimum_egreedy = np.empty((n_agents,n_trials))\n",
    "for iA in np.arange(n_agents): \n",
    "\n",
    "    reward_means = np.random.normal(0,1,(n_bandits,1))\n",
    "    optimal_action = np.argmax(reward_means)\n",
    "    reward_samples, reward_probs = generate_normal_bandit(n_trials,reward_means)\n",
    "\n",
    "    #initialize the estimated values for each bandit\n",
    "    values = np.zeros((len(reward_means),1))\n",
    "    action_count = np.zeros((len(reward_means),1))\n",
    "    \n",
    "    for iT in np.arange(n_trials):    \n",
    "        #chose action\n",
    "        action = action_epsilon(values,epsilon)\n",
    "        action_count[action,0] =  action_count[action,0]+1\n",
    "        #calculate prediction error\n",
    "        PE = reward_samples[action,iT]-values[action,0]\n",
    "        #update value\n",
    "        values[action,0] = values[action,0] + (1/(iT+1))*PE\n",
    "        #store reward\n",
    "        rewards_egreedy[iA,iT] = reward_samples[action,iT]\n",
    "        if action == optimal_action:\n",
    "            optimum_egreedy[iA,iT] = 1\n",
    "        else:\n",
    "            optimum_egreedy[iA,iT] = 0     \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39b012c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(12, 6))\n",
    "ax = fig.add_subplot(121)\n",
    "ax.plot(np.arange(n_trials),np.mean(rewards_greedy,0),'-k',label='greedy')\n",
    "ax.plot(np.arange(n_trials),np.mean(rewards_egreedy,0),'-r',label='e-greedy')\n",
    "plt.legend(loc=\"lower right\") \n",
    "ax.set_xlabel('trial')\n",
    "ax.set_ylabel('average reward')\n",
    "ax2 = fig.add_subplot(122)\n",
    "ax2.plot(np.arange(n_trials),np.mean(optimum_greedy,0),'-k',label='greedy')\n",
    "ax2.plot(np.arange(n_trials),np.mean(optimum_egreedy,0),'-r',label='e-greedy')\n",
    "plt.legend(loc=\"lower right\")   \n",
    "ax2.set_xlabel('trial')\n",
    "ax2.set_ylabel('% optimal choice')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "389943db",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\" role=\"alert\">\n",
    "<h2> Challenge </h2><br>\n",
    "\n",
    "It seems that something is wrong with the code. Can you figure out why the results of our code do not replicate the simulations results of Sutton & Barto? In silico, the $\\epsilon$-greedy method should outperform the greedy method for this environment. However it does not.\n",
    "    \n",
    ">HINT: Have a look at the updating function.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75905f1d",
   "metadata": {},
   "source": [
    "### Softmax action selection\n",
    "\n",
    "Let's have a closer look at the softmax rule, as this is maybe the most widely used decision rule in the neuroscientific literature.\n",
    "\n",
    "$\\LARGE p(a)= \\frac{e ^{(\\beta * Q(a))}} {\\sum \\limits _{a'} e ^{(\\beta * Q(a'))}}$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "79a16300",
   "metadata": {},
   "outputs": [],
   "source": [
    "def action_softm(values,beta):\n",
    "    nOptions = len(values)\n",
    "    actions = np.arange(nOptions)\n",
    "    \n",
    "    prob = ???\n",
    "    action = np.random.choice(actions,size = 1, p = prob) \n",
    "    \n",
    "    return action, prob"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f015c76",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\" role=\"alert\">\n",
    "<h2> Challenge </h2><br>\n",
    "\n",
    "The central part of the softmax algorithm is missing.\n",
    "    \n",
    ">HINT: Use prespecified numpy methods.\n",
    "</div>\n",
    "\n",
    "If we want to put this softmax function to use, we can make a few hypothetical assumptions for our bandit task.\n",
    "\n",
    "For example, let's assume our agent has played the slot machine twice, each arm one time. The left arm lead to a reward (1), whereas the right arm did not result in a reward (0). We can translate this experience into simplified expectations (values) for the next game."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c81a8dc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chosen action: 1\n",
      "Action probablities: [0.73105858 0.26894142]\n"
     ]
    }
   ],
   "source": [
    "dummy_values = np.array([1,0])\n",
    "beta = 1\n",
    "\n",
    "action,prob = action_softm(dummy_values,beta)\n",
    "\n",
    "print('Chosen action: %d' % (action))\n",
    "print('Action probablities: %s' % (np.array2string(prob)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc554ab5",
   "metadata": {},
   "source": [
    "We can see, that the dummy expectations translate into action probabilites in a straightforward way. The highest value has the highest action probability.\n",
    "\n",
    "<div class=\"alert alert-warning\" role=\"alert\">\n",
    "<h2> Idea </h2><br>\n",
    "Play around with the beta value and observe the changes!\n",
    "    \n",
    "</div>\n",
    "\n",
    "As already discussed in the lecture, the beta (or 'inverse temperature') affects the so-called [gain](https://en.wikipedia.org/wiki/Gain_(electronics)). The higher the gain, the more pronounced the differences in action values get translated into action probabilities.\n",
    "\n",
    "We can plot this in a systematic way.\n",
    "\n",
    "<div class=\"alert alert-warning\" role=\"alert\">\n",
    "<h2> Challenge </h2><br>\n",
    "Setup a plot similar to slide 17 in the lecture.<br>\n",
    "- x-axis: differences in values<br>\n",
    "- y-axis: action probablity for option 1<br>\n",
    "    \n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5982f234",
   "metadata": {},
   "outputs": [],
   "source": [
    "betas = [0,1,2,5,10]\n",
    "x = np.arange(-2,2,0.1)\n",
    "\n",
    "all_probs = np.empty((len(betas),len(x)))\n",
    "\n",
    "## TODO: calculate action probabilites for each [beta,x] pair\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "3b84c752",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## TODO: plot the results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dccac088",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\" role=\"alert\">\n",
    "<h2> Assignments and more </h2><br>\n",
    "Congratulations! You have reached the end of the notebook. I hope you have learned something about reinforcement learning.\n",
    "    \n",
    "Now it is time for your assignment. You find the assignments, additional material in the \"solution\" notebook.\n",
    "    \n",
    "Feel free to discuss your own ideas for assignments with the workgroup teacher!\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abe255b0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
